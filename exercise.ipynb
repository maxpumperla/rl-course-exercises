{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a4032f",
   "metadata": {},
   "source": [
    "\n",
    "Exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22052270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 4 from module 5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ____ import ____\n",
    "\n",
    "# suppress warnings\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build()\n",
    "\n",
    "ppo_rewards = []\n",
    "for i in range(10):\n",
    "    out = ppo.train()\n",
    "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "\n",
    "dqn_config = (\n",
    "    ____()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "dqn = dqn_config.build()\n",
    "\n",
    "dqn_rewards = []\n",
    "for i in range(10):\n",
    "    out = ____.train()\n",
    "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "plt.plot(ppo_rewards, label=\"PPO\")\n",
    "plt.plot(dqn_rewards, label=\"DQN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"training iterations\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac31607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# solution for exercise number 4 from module 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "# suppress warnings\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build()\n",
    "\n",
    "ppo_rewards = []\n",
    "for i in range(10):\n",
    "    out = ppo.train()\n",
    "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "\n",
    "dqn_config = (\n",
    "    DQNConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "dqn = dqn_config.build()\n",
    "\n",
    "dqn_rewards = []\n",
    "for i in range(10):\n",
    "    out = dqn.train()\n",
    "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "plt.plot(ppo_rewards, label=\"PPO\")\n",
    "plt.plot(dqn_rewards, label=\"DQN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"training iterations\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bad573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 9 from module 5\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=____)\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.____(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    ____              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "____.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36386874",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# solution for exercise number 9 from module 5\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "analysis.results_df[[\"config/lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092316c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 13 from module 5\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .____(____)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .____(num_rollout_workers=1, num_envs_per_worker=1)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a840c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# solution for exercise number 13 from module 5\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
