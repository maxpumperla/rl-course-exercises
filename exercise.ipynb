{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc179e4",
   "metadata": {},
   "source": [
    "\n",
    "Exercises and solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 6 from module 1\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "\n",
    "# Add calls to `step` here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684262d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 6 from module 1\n",
    "import gym\n",
    "\n",
    "taxi = gym.make(\"Taxi-v3\")\n",
    "obs = taxi.reset(seed=5)\n",
    "taxi.render()\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.step(1)\n",
    "taxi.render()\n",
    "taxi.step(4)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.step(0)\n",
    "taxi.render()\n",
    "obs, reward, done, _ = taxi.step(5)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cff1f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 7 from module 1\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", \n",
    "               desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), \n",
    "               is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = ____\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02fc268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 7 from module 1\n",
    "\n",
    "import gym.envs.toy_text\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=gym.envs.toy_text.frozen_lake.generate_random_map(size=3, p=0.3), is_slippery=False)\n",
    "env.render = None\n",
    "\n",
    "obs = env.reset()\n",
    "actions = []\n",
    "# BEGIN SOLUTION\n",
    "actions = [1,2,1,2]\n",
    "# END SOLUTION\n",
    "for action in actions:\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(\"Obs:\", obs, \"Reward:\", reward, \"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d02a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 10 from module 1\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "for ____:\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while ____:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(____)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b82a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 10 from module 1\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000): # loop over episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.randint(low=0, high=4)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 11 from module 1\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 1,\n",
    "    5 : 1,\n",
    "    6 : 1,\n",
    "    7 : 1,\n",
    "    8 : 2,\n",
    "    9 : 2,\n",
    "    10: 2,\n",
    "    11: 0,\n",
    "    12: 2,\n",
    "    13: 2,\n",
    "    14: 2,\n",
    "    15: 2\n",
    "}\n",
    "\n",
    "rewards = []\n",
    "N = 1000\n",
    "for i in range(N): # loop over episodes\n",
    "\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    rewards.append(reward)\n",
    "    \n",
    "print(\"Average reward:\", sum(rewards)/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33192b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 3 from module 2\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=____, config=slippery_algo_config)\n",
    "\n",
    "for i in range(____):\n",
    "    train_info = ppo.____()\n",
    "    \n",
    "eval_results = ____.evaluate()\n",
    "\n",
    "print(eval_results[\"evaluation\"][\"episode_reward_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d028e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 3 from module 2\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from utils import slippery_algo_config\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "\n",
    "for i in range(50): # There is randomness here, but 20+ should be enough most of the time\n",
    "    train_info = ppo.train()\n",
    "    \n",
    "eval_results = ppo.evaluate()\n",
    "\n",
    "print(\"Frequency of reaching goal: %.1f%%\" % (eval_results[\"evaluation\"][\"episode_reward_mean\"]*100))\n",
    "\n",
    "print(\"Action performed from top-right:\", ppo.compute_single_action(3, explore=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 4 from module 2\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from utils import slippery_algo_config\n",
    "import gym\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.seed(12)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True);\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 4 from module 2\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from utils import slippery_algo_config\n",
    "import gym\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "ppo = PPO(env=\"FrozenLake-v1\", config=slippery_algo_config)\n",
    "ppo.restore(\"models/FrozenLakeSlippery50/checkpoint_000050/\")\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "\n",
    "obs = env.reset()\n",
    "env.seed(12)\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs, explore=False)\n",
    "    obs, rewards, done, _ = env.step(action)\n",
    "\n",
    "    display.clear_output(wait=True);\n",
    "    print(env.render(mode=\"ansi\"))\n",
    "    time.sleep(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 8 from module 2\n",
    "# SOLUTION\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from envs import MyCartPole\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "config = (\n",
    "    ____()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo = ____.____(env=MyCartPole);\n",
    "\n",
    "____.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
    "\n",
    "env = MyCartPole()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "\n",
    "display.clear_output(wait=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58383d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 8 from module 2\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from envs import MyCartPole\n",
    "from ray.rllib.algorithms.ppo import PPO, PPOConfig\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo = config.build(env=MyCartPole);\n",
    "\n",
    "ppo.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
    "\n",
    "env = MyCartPole()\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = ppo.compute_single_action(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "\n",
    "display.clear_output(wait=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a64dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 9 from module 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import envs\n",
    "\n",
    "cartpole_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env=envs.MyCartPole)\n",
    ")\n",
    "\n",
    "ppo = cartpole_config.build();\n",
    "\n",
    "ppo.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
    "\n",
    "angle_range_deg = np.arange(-15,15,0.1)\n",
    "push_left_probs = 0*angle_range_deg\n",
    "\n",
    "env = envs.MyCartPole()\n",
    "obs = env.reset()\n",
    "for i, angle_deg in enumerate(angle_range_deg):\n",
    "    angle_rad = angle_deg/180*np.pi\n",
    "    \n",
    "    obs = np.zeros(4)\n",
    "    obs[2] = angle_rad\n",
    "\n",
    "    push_left_probs[i] = utils.query_policy(ppo, env, obs, actions=[0,1])[0]\n",
    "\n",
    "plt.plot(angle_range_deg, push_left_probs);\n",
    "plt.xlabel(\"pole angle (degrees)\")\n",
    "plt.ylabel(\"probability of pushing left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c8ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 9 from module 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "import envs\n",
    "\n",
    "cartpole_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    "    .environment(env=envs.MyCartPole)\n",
    ")\n",
    "\n",
    "ppo = cartpole_config.build();\n",
    "\n",
    "ppo.restore(\"models/CartPole-Ray2/checkpoint_000050\")\n",
    "\n",
    "angle_range_deg = np.arange(-15,15,0.1)\n",
    "push_left_probs = 0*angle_range_deg\n",
    "\n",
    "env = envs.MyCartPole()\n",
    "obs = env.reset()\n",
    "for i, angle_deg in enumerate(angle_range_deg):\n",
    "    angle_rad = angle_deg/180*np.pi\n",
    "    \n",
    "    obs = np.zeros(4)\n",
    "    obs[2] = angle_rad\n",
    "\n",
    "    push_left_probs[i] = utils.query_policy(ppo, env, obs, actions=[0,1])[0]\n",
    "\n",
    "plt.plot(angle_range_deg, push_left_probs);\n",
    "plt.xlabel(\"pole angle (degrees)\")\n",
    "plt.ylabel(\"probability of pushing left\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 13 from module 2\n",
    "\n",
    "from envs import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    "    .multi_agent(\n",
    "        ____=[\"policy1\", \"policy2\"],\n",
    "        ____=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "ppo_arena = ppo_config.build(env=MultiAgentArena)\n",
    "\n",
    "ppo_arena.restore(\"models/MultiAgent20/checkpoint_000020\")\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "    \n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = ppo_arena.compute_single_action(____, policy_id=\"policy1\")\n",
    "    action2 = ppo_arena.compute_single_action(____, policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, infos = env.step({\"agent1\": ____, \"agent2\": ____})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c566e73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 13 from module 2\n",
    "\n",
    "from envs import MultiAgentArena\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]})\\\n",
    "    .multi_agent(\n",
    "        policies=[\"policy1\", \"policy2\"],\n",
    "        policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "ppo_arena = ppo_config.build(env=MultiAgentArena)\n",
    "\n",
    "ppo_arena.restore(\"models/MultiAgent20/checkpoint_000020\")\n",
    "\n",
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "dones = {\"__all__\" : False}\n",
    "    \n",
    "while not dones[\"__all__\"]:\n",
    "\n",
    "    action1 = ppo_arena.compute_single_action(obs[\"agent1\"], policy_id=\"policy1\")\n",
    "    action2 = ppo_arena.compute_single_action(obs[\"agent2\"], policy_id=\"policy2\")\n",
    "\n",
    "    obs, rewards, dones, infos = env.step({\"agent1\": action1, \"agent2\": action2})\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7814a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 3 from module 3\n",
    "\n",
    "class Maze(FrozenPond):\n",
    "    def done(self):\n",
    "        return self.player == self.goal or self.holes[self.player] == 1\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 3 from module 3\n",
    "\n",
    "class Maze(FrozenPond):   \n",
    "    def done(self):\n",
    "        return self.player == self.goal\n",
    "    def is_valid_loc(self, location):\n",
    "        if 0 <= location[0] <= 3 and 0 <= location[1] <= 3 and not self.holes[location]:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "pond = FrozenPond()\n",
    "pond.reset()\n",
    "pond.step(1)\n",
    "print(pond.step(2))\n",
    "\n",
    "maze = Maze()\n",
    "maze.reset()\n",
    "maze.step(1)\n",
    "print(maze.step(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 8 from module 3\n",
    "\n",
    "import gym\n",
    "\n",
    "class RandomLake(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)      \n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.random.rand(4, 4) < 0.2\n",
    "        self.holes[self.player] = 0  # no hole at start location\n",
    "        self.holes[self.goal] = 0    # no hole at goal location\n",
    "        \n",
    "        self.stepcount = 0\n",
    "        \n",
    "        return self.observation()\n",
    "    \n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]\n",
    "    \n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)\n",
    "    \n",
    "    def done(self):\n",
    "        is_done = self.player == self.goal or self.holes[self.player] == 1 \n",
    "        return is_done\n",
    "    \n",
    "    def is_valid_loc(self, location):\n",
    "        return 0 <= location[0] <= 3 and 0 <= location[1] <= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "            \n",
    "        self.stepcount += 1\n",
    "        \n",
    "        return self.observation(), self.reward(), self.done(), {}\n",
    "    \n",
    "lake = RandomLake()\n",
    "obs = lake.reset()\n",
    "\n",
    "done = False\n",
    "for i in range(55):\n",
    "    obs, rewards, done, _ = lake.step(0)\n",
    "    print(i+1, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 8 from module 3\n",
    "\n",
    "import gym\n",
    "\n",
    "class RandomLake(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        self.observation_space = gym.spaces.Discrete(16)\n",
    "        self.action_space = gym.spaces.Discrete(4)      \n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (0, 0) # the player starts at the top-left\n",
    "        self.goal = (3, 3)   # goal is at the bottom-right\n",
    "        \n",
    "        self.holes = np.random.rand(4, 4) < 0.2\n",
    "        self.holes[self.player] = 0  # no hole at start location\n",
    "        self.holes[self.goal] = 0    # no hole at goal location\n",
    "        \n",
    "        self.stepcount = 0\n",
    "        \n",
    "        return self.observation()\n",
    "    \n",
    "    def observation(self):\n",
    "        return 4*self.player[0] + self.player[1]\n",
    "    \n",
    "    def reward(self):\n",
    "        return int(self.player == self.goal)\n",
    "    \n",
    "    def done(self):\n",
    "        is_done = self.player == self.goal or self.holes[self.player] == 1 or self.stepcount >= 50\n",
    "        return is_done\n",
    "    \n",
    "    def is_valid_loc(self, location):\n",
    "        return 0 <= location[0] <= 3 and 0 <= location[1] <= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        # Compute the new player location\n",
    "        if action == 0:   # left\n",
    "            new_loc = (self.player[0], self.player[1]-1)\n",
    "        elif action == 1: # down\n",
    "            new_loc = (self.player[0]+1, self.player[1])\n",
    "        elif action == 2: # right\n",
    "            new_loc = (self.player[0], self.player[1]+1)\n",
    "        elif action == 3: # up\n",
    "            new_loc = (self.player[0]-1, self.player[1])\n",
    "        else:\n",
    "            raise ValueError(\"Action must be in {0,1,2,3}\")\n",
    "        \n",
    "        # Update the player location only if you stayed in bounds\n",
    "        if self.is_valid_loc(new_loc):\n",
    "            self.player = new_loc\n",
    "            \n",
    "        self.stepcount += 1\n",
    "        \n",
    "        return self.observation(), self.reward(), self.done(), {}\n",
    "    \n",
    "lake = RandomLake()\n",
    "obs = lake.reset()\n",
    "\n",
    "done = False\n",
    "for i in range(55):\n",
    "    obs, rewards, done, _ = lake.step(0)\n",
    "    print(i+1, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b792f3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 13 from module 3\n",
    "\n",
    "from envs import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(1 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(1 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(1 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(1 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb558c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 13 from module 3\n",
    "\n",
    "from envs import RandomLake\n",
    "\n",
    "class RandomLakeObs2(RandomLakeObs):\n",
    "    def observation(self):\n",
    "        i, j = self.player\n",
    "\n",
    "        obs = []\n",
    "        obs.append(2 if j==0 else self.holes[i,j-1]) # left\n",
    "        obs.append(2 if i==3 else self.holes[i+1,j]) # down\n",
    "        obs.append(2 if j==3 else self.holes[i,j+1]) # right\n",
    "        obs.append(2 if i==0 else self.holes[i-1,j]) # up\n",
    "        \n",
    "        obs = np.array(obs, dtype=int) # cast to numpy array\n",
    "        return obs\n",
    "\n",
    "np.random.seed(42)\n",
    "env = RandomLakeObs2()\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 14 from module 3\n",
    "\n",
    "import numpy as np\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 14 from module 3\n",
    "\n",
    "import numpy as np\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "actions = {\"left\" : 0, \"down\" : 1, \"right\" : 2, \"up\" : 3, \n",
    "           \"l\" : 0, \"d\" : 1, \"r\" : 2, \"u\" : 3}\n",
    "\n",
    "np.random.seed(45)\n",
    "env = RandomLakeObs()\n",
    "obs = env.reset()\n",
    "\n",
    "act = \"start\"\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "   \n",
    "    obs_print = [['.']*3 for i in range(3)]\n",
    "    obs_print[1][1] = \"P\"\n",
    "    if obs[0]:\n",
    "        obs_print[1][0] = \"O\"\n",
    "    if obs[1]:\n",
    "        obs_print[2][1] = \"O\"\n",
    "    if obs[2]:\n",
    "        obs_print[1][2] = \"O\"\n",
    "    if obs[3]:\n",
    "        obs_print[0][1] = \"O\"\n",
    "    print(\"Observation:\")\n",
    "    print(\"\\n\".join(list(map(lambda c: \"\".join(c), obs_print))))\n",
    "    print()\n",
    "    \n",
    "    while act != \"quit\" and act not in actions: \n",
    "        act = input() # gather keyboard input \n",
    "    \n",
    "    if act == \"quit\":\n",
    "        break\n",
    "        \n",
    "    obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "if done:\n",
    "    if rew > 0:\n",
    "        print(\"You win! +1 reward 🎉\")\n",
    "    else:\n",
    "        print(\"You fell into the lake 😢\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 19 from module 3\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return ____\n",
    "    \n",
    "ppo = lake_default_config.build(env=____)\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][____])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][____])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e99ae3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 19 from module 3\n",
    "from utils import lake_default_config\n",
    "from envs import RandomLakeObs\n",
    "\n",
    "class RandomLakeBadIdea(RandomLakeObs):\n",
    "    def reward(self):\n",
    "        old_reward = int(self.player == self.goal) \n",
    "        return old_reward - 1\n",
    "\n",
    "ppo = lake_default_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "\n",
    "for i in range(8):\n",
    "    ppo.train()\n",
    "    \n",
    "print(\"Average episode length for trained agent: %.1f\" % \n",
    "      ppo.evaluate()[\"evaluation\"][\"episode_len_mean\"])\n",
    "\n",
    "random_agent_config = (\n",
    "    lake_default_config\\\n",
    "    .exploration(exploration_config={\"type\": \"Random\"})\\\n",
    "    .evaluation(evaluation_config={\"explore\" : True})\n",
    ")\n",
    "random_agent = random_agent_config.build(env=RandomLakeBadIdea)\n",
    "\n",
    "print(\"Average episode length for random agent: %.1f\" % \n",
    "      random_agent.evaluate()[\"evaluation\"][\"episode_len_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14614e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 8 from module 4\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0, 0, 0]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 8 from module 4\n",
    "\n",
    "def update_sugar_level(sugar_level, item_sweetness, alpha=0.9):\n",
    "    return alpha * sugar_level + (1 - alpha) * item_sweetness\n",
    "\n",
    "def reward(sugar_level, item_sweetness):\n",
    "    return item_sweetness * (1 - sugar_level)\n",
    "\n",
    "# MODIFY THIS LIST\n",
    "# But make sure it always contains 3 items, each 0 or 1\n",
    "recommendations = [0,1,1]\n",
    "\n",
    "# starting sugar level\n",
    "sugar_level = 0.5\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for item_sweetness in recommendations:\n",
    "    \n",
    "    # add reward\n",
    "    immediate_reward = reward(sugar_level, item_sweetness)\n",
    "    total_reward += immediate_reward\n",
    "    \n",
    "    # update sugar level\n",
    "    sugar_level = update_sugar_level(sugar_level, item_sweetness, alpha=0.7)\n",
    "    \n",
    "    print(f\"  Received reward {immediate_reward:.5f}, new sugar level {sugar_level:.5f}\")\n",
    "    \n",
    "print(\"Total reward after 5 recommendations\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5920a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 12 from module 4\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from envs import BasicRecommender\n",
    "\n",
    "def baseline_episode(env, method=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Compute the episode reward for a BasicRecommender env by either\n",
    "    acting greedy (max observation) or acting randomly.\n",
    "    Return total reward.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if method == \"greedy\":\n",
    "            chosen_item = np.argmax(____)\n",
    "        elif method == \"random\":\n",
    "            chosen_item = np.random.randint(____)\n",
    "        else:\n",
    "            raise Exception(\"Unknown method.\")\n",
    "        obs, reward, done, info = env.____(chosen_item)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def baseline_multiple_episodes(env, method, n_ep=100):\n",
    "    \"\"\" Compute baseline reward averaged over multiple episodes \"\"\"\n",
    "    return np.mean([baseline_episode(env, method) for ep in range(n_ep)])\n",
    "\n",
    "max_steps = [1,3,10,30,100] # The different horizon lengths to test out\n",
    "greedy_results = []\n",
    "random_results = []\n",
    "\n",
    "# Run the simulations\n",
    "for ms in ____:\n",
    "    env = BasicRecommender({\"max_steps\" : ____})\n",
    "    greedy_results.append(baseline_multiple_episodes(env, \"greedy\"))\n",
    "    random_results.append(baseline_multiple_episodes(env, \"random\"))\n",
    "    \n",
    "# Plotting code (you can ignore)\n",
    "plt.plot(max_steps, greedy_results, label=\"greedy\")\n",
    "plt.plot(max_steps, random_results, label=\"random\")\n",
    "plt.legend();\n",
    "plt.xlabel(\"horizon of the env\");\n",
    "plt.ylabel(\"episode reward\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac292a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 12 from module 4\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from envs import BasicRecommender\n",
    "\n",
    "def baseline_episode(env, method=\"greedy\"):\n",
    "    \"\"\"\n",
    "    Compute the episode reward for a BasicRecommender env by either\n",
    "    acting greedy (max observation) or acting randomly.\n",
    "    Return total reward.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        if method == \"greedy\":\n",
    "            chosen_item = np.argmax(obs)\n",
    "        elif method == \"random\":\n",
    "            chosen_item = np.random.randint(len(obs))\n",
    "        else:\n",
    "            raise Exception(\"Unknown method.\")\n",
    "        obs, reward, done, info = env.step(chosen_item)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "\n",
    "def baseline_multiple_episodes(env, method, n_ep=100):\n",
    "    \"\"\" Compute baseline reward averaged over multiple episodes \"\"\"\n",
    "    return np.mean([baseline_episode(env, method) for ep in range(n_ep)])\n",
    "\n",
    "max_steps = [1,3,10,30,100] # The different horizon lengths to test out\n",
    "greedy_results = []\n",
    "random_results = []\n",
    "\n",
    "# Run the simulations\n",
    "for ms in max_steps:\n",
    "    env = BasicRecommender({\"max_steps\" : ms})\n",
    "    greedy_results.append(baseline_multiple_episodes(env, \"greedy\"))\n",
    "    random_results.append(baseline_multiple_episodes(env, \"random\"))\n",
    "    \n",
    "# Plotting code (you can ignore)\n",
    "plt.plot(max_steps, greedy_results, label=\"greedy\")\n",
    "plt.plot(max_steps, random_results, label=\"random\")\n",
    "plt.legend();\n",
    "plt.xlabel(\"horizon of the env\");\n",
    "plt.ylabel(\"episode reward\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547cf5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 15 from module 4\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from envs import BasicRecommender\n",
    "from utils import query_policy\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = ( # change \"gamma\" in here\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001, gamma=0.99)\\\n",
    "    .environment(env_config=env_config)\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build(env=BasicRecommender)\n",
    "\n",
    "rewards = []\n",
    "for i in range(5):\n",
    "    result = ppo.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "\n",
    "env = BasicRecommender(env_config)\n",
    "env.reset()\n",
    "print(query_policy(ppo, env, np.array([0,1]), actions=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 15 from module 4\n",
    "import numpy as np\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from envs import BasicRecommender\n",
    "from utils import query_policy\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = ( # change \"gamma\" in here\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, lr=0.001, gamma=0.99)\\\n",
    "    .environment(env_config=env_config)\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build(env=BasicRecommender)\n",
    "\n",
    "rewards = []\n",
    "for i in range(5):\n",
    "    result = ppo.train()\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "\n",
    "env = BasicRecommender(env_config)\n",
    "env.reset()\n",
    "print(query_policy(ppo, env, np.array([0,1]), actions=[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bac00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 20 from module 4\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "\n",
    "offline_trainer_config = {\n",
    "    # These should look familiar:\n",
    "    \"framework\"             : \"torch\",\n",
    "    \"create_env_on_driver\"  : True,\n",
    "    \"seed\"                  : 0,\n",
    "    \"model\"                 : {\n",
    "        \"fcnet_hiddens\"     : [64, 64]\n",
    "    },\n",
    "    \n",
    "    # These are new for offline RL:\n",
    "    ____: [\"data/cartpolev1_offline.json\"],\n",
    "    \"observation_space\": gym.spaces.Box(low=____, \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    \"action_space\": gym.spaces.Discrete(2),\n",
    "    \"input_evaluation\" : [\"simulation\", \"is\", \"wis\"],\n",
    "    \"env\" : \"CartPole-v1\" # for evaluation only\n",
    "}\n",
    "\n",
    "trainer = MARWILTrainer(config=____)\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(200):\n",
    "    r = trainer.____()\n",
    "    results_off.append(r[\"off_policy_estimator\"][\"wis\"]['V_gain_est'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eabd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 20 from module 4\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.marwil import MARWIL, MARWILConfig\n",
    "from ray.rllib.algorithms.crr import CRR, CRRConfig\n",
    "\n",
    "# This is the same as before\n",
    "offline_config = ( \n",
    "    CRRConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\\\n",
    ")\n",
    "# This is new for offline RL\n",
    "offline_config = offline_config.environment(\n",
    "    observation_space = gym.spaces.Box(low=np.array([-4.8, -np.inf, -0.42, -np.inf]), \n",
    "                                        high=np.array([4.8,  np.inf,  0.42,  np.inf])),\n",
    "    action_space = gym.spaces.Discrete(2),\n",
    "    env = \"CartPole-v1\" # for evaluation only\n",
    ").offline_data(\n",
    "    input_ = [\"data/cartpolev1_offline.json\"]\n",
    ").evaluation(\n",
    "    off_policy_estimation_methods={'simulation': {'type': 'simulation'}}\n",
    ")\n",
    "\n",
    "algo = offline_config.build()\n",
    "\n",
    "# Training (and storing results)\n",
    "results_off = []\n",
    "results_sim = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    r = algo.train()\n",
    "    results_off.append(r[\"\"][\"wis\"]['v_new_mean'])\n",
    "    results_sim.append(r[\"episode_reward_mean\"])\n",
    "\n",
    "plt.plot(results_sim);\n",
    "plt.xlabel('iterations') \n",
    "plt.ylabel('simulator reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52ffd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 4 from module 5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ____ import ____\n",
    "\n",
    "# suppress warnings\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build()\n",
    "\n",
    "ppo_rewards = []\n",
    "for i in range(10):\n",
    "    out = ppo.train()\n",
    "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "\n",
    "dqn_config = (\n",
    "    ____()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "dqn = dqn_config.build()\n",
    "\n",
    "dqn_rewards = []\n",
    "for i in range(10):\n",
    "    out = ____.train()\n",
    "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "plt.plot(ppo_rewards, label=\"PPO\")\n",
    "plt.plot(dqn_rewards, label=\"DQN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"training iterations\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 4 from module 5\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "# suppress warnings\n",
    "import ray\n",
    "import logging\n",
    "ray.init(log_to_driver=False, ignore_reinit_error=True, logging_level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "ppo = ppo_config.build()\n",
    "\n",
    "ppo_rewards = []\n",
    "for i in range(10):\n",
    "    out = ppo.train()\n",
    "    ppo_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "\n",
    "dqn_config = (\n",
    "    DQNConfig()\n",
    "    .framework(\"torch\")\n",
    "    .rollouts(create_env_on_local_worker=True)\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\n",
    "    .training(model={\"fcnet_hiddens\" : [32, 32]})\n",
    "    .environment(env=\"Taxi-v3\")\n",
    ")\n",
    "\n",
    "dqn = dqn_config.build()\n",
    "\n",
    "dqn_rewards = []\n",
    "for i in range(10):\n",
    "    out = dqn.train()\n",
    "    dqn_rewards.append(out[\"episode_reward_mean\"])\n",
    "    \n",
    "plt.plot(ppo_rewards, label=\"PPO\")\n",
    "plt.plot(dqn_rewards, label=\"DQN\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"training iterations\")\n",
    "plt.ylabel(\"reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc1ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 9 from module 5\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=____)\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.____(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    ____              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "____.results_df[[\"lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 9 from module 5\n",
    "\n",
    "from envs import BasicRecommenderWithHistory\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray import tune\n",
    "\n",
    "env_config = {\n",
    "    \"num_candidates\" : 2,\n",
    "    \"alpha\"          : 0.5,\n",
    "    \"seed\"           : 42\n",
    "}\n",
    "\n",
    "ppo_config = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(create_env_on_local_worker=True)\\\n",
    "    .debugging(seed=0, log_level=\"ERROR\")\\\n",
    "    .training(model={\"fcnet_hiddens\" : [64, 64]}, \n",
    "              lr=tune.grid_search([1e-2, 1e-3, 1e-4]))\\\n",
    "    .environment(env_config=env_config, env=BasicRecommenderWithHistory)\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    config            = ppo_config.to_dict(),\n",
    "    stop              = {\"training_iteration\" : 10},\n",
    "    checkpoint_freq   = 1,\n",
    "    verbose           = 0,\n",
    "    metric            = \"episode_reward_mean\",\n",
    "    mode              = \"max\"\n",
    ")\n",
    "\n",
    "analysis.results_df[[\"config/lr\", \"episode_reward_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc5497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# exercise number 13 from module 5\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .____(____)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .____(num_rollout_workers=1, num_envs_per_worker=1)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f810771c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# #@title solution for exercise number 13 from module 5\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "ppo_config_many = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=2)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_config_single = (\n",
    "    PPOConfig()\\\n",
    "    .framework(\"torch\")\\\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\\\n",
    "    .training(model={\"fcnet_hiddens\" : [32,32]})\n",
    ")\n",
    "\n",
    "ppo_many = ppo_config_many.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_many.train()\n",
    "print(f\"Elapsed time with 2 workers, 2 envs each: {time.time()-t:.1f}s.\")\n",
    "ppo_many.stop()\n",
    "\n",
    "ppo_single = ppo_config_single.build(env=\"FrozenLake-v1\")\n",
    "t = time.time()\n",
    "for i in range(5):\n",
    "    ppo_single.train()\n",
    "print(f\"Elapsed time with 1 worker, 1 env: {time.time()-t:.1f}s.\")\n",
    "ppo_single.stop()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
